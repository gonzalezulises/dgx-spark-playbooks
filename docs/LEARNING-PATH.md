# Plan de Aprendizaje: DGX Spark Playbooks

> Ruta de aprendizaje √≥ptima para dominar los 33 laboratorios de DGX Spark
> 
> **Autor**: Ulises Gonz√°lez - Rizo.ma  
> **Fecha**: 2026-01-16  
> **Duraci√≥n Total Estimada**: 40-50 horas

---

## Filosof√≠a del Plan

Este plan est√° dise√±ado siguiendo principios pedag√≥gicos:

1. **Fundamentos Primero**: Dominar la infraestructura antes de las aplicaciones
2. **Complejidad Progresiva**: De lo simple a lo complejo
3. **Dependencias Respetadas**: Cada lab prepara para el siguiente
4. **Pr√°ctica Inmediata**: Cada concepto se aplica antes de avanzar
5. **Consolidaci√≥n**: Labs relacionados se agrupan para reforzar aprendizaje

---

## Mapa Visual del Curriculum

```
                            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                            ‚îÇ         NIVEL 6: MULTI-NODE             ‚îÇ
                            ‚îÇ    Connect Two Sparks ‚Üí NCCL ‚Üí Dist     ‚îÇ
                            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                              ‚ñ≤
                            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                            ‚îÇ      NIVEL 5: ESPECIALIZACI√ìN           ‚îÇ
                            ‚îÇ  Isaac Sim ‚îÇ Diffusion ‚îÇ Bioinformtic  ‚îÇ
                            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                              ‚ñ≤
                            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                            ‚îÇ        NIVEL 4: FINE-TUNING             ‚îÇ
                            ‚îÇ  NeMo ‚Üí Unsloth ‚Üí LLaMA Factory ‚Üí FLUX  ‚îÇ
                            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                              ‚ñ≤
                            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                            ‚îÇ      NIVEL 3: RAG & AGENTES             ‚îÇ
                            ‚îÇ   RAG Workbench ‚Üí txt2kg ‚Üí Multi-Agent  ‚îÇ
                            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                              ‚ñ≤
                            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                            ‚îÇ       NIVEL 2: INFERENCIA               ‚îÇ
                            ‚îÇ  Ollama ‚Üí vLLM ‚Üí TRT-LLM ‚Üí NIM ‚Üí SGLang ‚îÇ
                            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                              ‚ñ≤
                            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                            ‚îÇ       NIVEL 1: FUNDAMENTOS              ‚îÇ
                            ‚îÇ  Setup ‚Üí Dashboard ‚Üí VS Code ‚Üí Tailscale‚îÇ
                            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## NIVEL 1: Fundamentos de Infraestructura
> **Objetivo**: Dominar el acceso, monitoreo y desarrollo remoto en DGX Spark  
> **Duraci√≥n**: 3-4 horas  
> **Prerequisitos**: Ninguno

### Lab 1.1: Connect to Your Spark
| Aspecto | Detalle |
|---------|---------|
| **Tiempo** | 30 min |
| **Objetivo** | Configurar acceso de red local al DGX Spark |
| **Aprender√°s** | Networking b√°sico, SSH, configuraci√≥n IP |
| **Entregable** | Acceso SSH funcional desde tu m√°quina local |

```bash
# Verificaci√≥n de √©xito
ssh usuario@dgx-spark.local
nvidia-smi  # Debe mostrar la GPU
```

### Lab 1.2: DGX Dashboard
| Aspecto | Detalle |
|---------|---------|
| **Tiempo** | 30 min |
| **Objetivo** | Monitorear recursos del sistema en tiempo real |
| **Aprender√°s** | M√©tricas GPU/CPU/RAM, utilizaci√≥n, temperatura |
| **Entregable** | Dashboard accesible v√≠a web browser |

**Por qu√© es importante**: Necesitas monitorear recursos antes de ejecutar cargas pesadas.

### Lab 1.3: VS Code Remote
| Aspecto | Detalle |
|---------|---------|
| **Tiempo** | 30 min |
| **Objetivo** | Desarrollo remoto con VS Code |
| **Aprender√°s** | Remote-SSH extension, workspace remoto |
| **Entregable** | VS Code conectado al DGX Spark |

**Por qu√© es importante**: Ambiente de desarrollo c√≥modo para el resto de los labs.

### Lab 1.4: Tailscale VPN
| Aspecto | Detalle |
|---------|---------|
| **Tiempo** | 30 min |
| **Objetivo** | Acceso remoto seguro desde cualquier lugar |
| **Aprender√°s** | VPN mesh, configuraci√≥n Tailscale |
| **Entregable** | Acceso al DGX desde fuera de la red local |

**Por qu√© es importante**: Flexibilidad para trabajar desde cualquier ubicaci√≥n.

### Lab 1.5: Vibe Coding
| Aspecto | Detalle |
|---------|---------|
| **Tiempo** | 30 min |
| **Objetivo** | Coding asistido por AI en VS Code |
| **Aprender√°s** | GitHub Copilot / AI assistants en VS Code |
| **Entregable** | AI coding assistant funcional |

### üìä Checkpoint Nivel 1
```
‚ñ° Puedo acceder al DGX via SSH
‚ñ° Puedo monitorear GPU/CPU/RAM en dashboard
‚ñ° Tengo VS Code conectado remotamente
‚ñ° Puedo acceder desde fuera de mi red (Tailscale)
‚ñ° Tengo AI coding assistant configurado
```

---

## NIVEL 2: Inferencia de Modelos
> **Objetivo**: Dominar diferentes backends de inferencia LLM  
> **Duraci√≥n**: 6-8 horas  
> **Prerequisitos**: Nivel 1 completado

### Lab 2.1: Ollama ‚≠ê (Punto de Partida)
| Aspecto | Detalle |
|---------|---------|
| **Tiempo** | 30 min |
| **Objetivo** | Inferencia LLM simple y r√°pida |
| **Aprender√°s** | Gesti√≥n de modelos, API REST, streaming |
| **Entregable** | Ollama sirviendo modelos localmente |

```bash
# Verificaci√≥n de √©xito
ollama run llama3.2
curl http://localhost:11434/api/generate -d '{"model":"llama3.2","prompt":"Hello"}'
```

**Por qu√© primero**: Es el m√°s simple y sirve como baseline de comparaci√≥n.

### Lab 2.2: Open WebUI
| Aspecto | Detalle |
|---------|---------|
| **Tiempo** | 30 min |
| **Objetivo** | UI web completa para Ollama |
| **Aprender√°s** | Docker, UI chat, gesti√≥n de conversaciones |
| **Entregable** | ChatGPT-like interface para tus modelos |

**Dependencia**: Requiere Lab 2.1 (Ollama)

### Lab 2.3: vLLM
| Aspecto | Detalle |
|---------|---------|
| **Tiempo** | 45 min |
| **Objetivo** | Inferencia de alto throughput |
| **Aprender√°s** | PagedAttention, batching, OpenAI-compatible API |
| **Entregable** | vLLM sirviendo con mejor throughput que Ollama |

**Comparaci√≥n con Ollama**:
| M√©trica | Ollama | vLLM |
|---------|--------|------|
| Setup | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |
| Throughput | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| Memoria | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |

### Lab 2.4: TensorRT-LLM ‚ö†Ô∏è
| Aspecto | Detalle |
|---------|---------|
| **Tiempo** | 60 min |
| **Requisito** | NGC API Key |
| **Objetivo** | Inferencia optimizada con TensorRT |
| **Aprender√°s** | Compilaci√≥n de modelos, optimizaci√≥n GPU |
| **Entregable** | Modelo compilado con TRT corriendo |

**Por qu√© despu√©s de vLLM**: TRT-LLM requiere compilaci√≥n previa, m√°s complejo.

### Lab 2.5: NIM on Spark ‚ö†Ô∏è
| Aspecto | Detalle |
|---------|---------|
| **Tiempo** | 45 min |
| **Requisito** | NGC API Key |
| **Objetivo** | NVIDIA Inference Microservices |
| **Aprender√°s** | Contenedores NIM, deployment enterprise |
| **Entregable** | NIM container sirviendo Llama 3.1 8B |

### Lab 2.6: SGLang ‚ö†Ô∏è
| Aspecto | Detalle |
|---------|---------|
| **Tiempo** | 45 min |
| **Requisito** | HuggingFace Token |
| **Objetivo** | Structured Generation Language |
| **Aprender√°s** | Generaci√≥n estructurada, JSON schemas |
| **Entregable** | SGLang sirviendo con outputs estructurados |

### Lab 2.7: Nemotron
| Aspecto | Detalle |
|---------|---------|
| **Tiempo** | 30 min |
| **Objetivo** | Modelo NVIDIA optimizado con llama.cpp |
| **Aprender√°s** | llama.cpp, modelos GGUF |
| **Entregable** | Nemotron-3-Nano corriendo localmente |

### Lab 2.8: Speculative Decoding
| Aspecto | Detalle |
|---------|---------|
| **Tiempo** | 45 min |
| **Objetivo** | Acelerar inferencia con draft models |
| **Aprender√°s** | Draft/target model pattern, speedup |
| **Entregable** | Inferencia 2-3x m√°s r√°pida |

### Lab 2.9: NVFP4 Quantization
| Aspecto | Detalle |
|---------|---------|
| **Tiempo** | 45 min |
| **Objetivo** | Cuantizaci√≥n a 4-bit con NVIDIA |
| **Aprender√°s** | Quantization, trade-offs calidad/velocidad |
| **Entregable** | Modelo cuantizado corriendo eficientemente |

### üìä Checkpoint Nivel 2
```
‚ñ° Entiendo las diferencias entre backends de inferencia
‚ñ° Puedo elegir el backend correcto seg√∫n el caso de uso
‚ñ° S√© cu√°ndo usar Ollama vs vLLM vs TRT-LLM
‚ñ° Puedo optimizar inferencia con quantization
‚ñ° Entiendo speculative decoding
```

### Tabla Comparativa (Resultado del Nivel 2)

| Backend | Setup | Throughput | Latencia | Memoria | Caso de Uso |
|---------|-------|------------|----------|---------|-------------|
| Ollama | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | Desarrollo, prototipado |
| vLLM | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | Producci√≥n, alto tr√°fico |
| TRT-LLM | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | M√°ximo rendimiento |
| NIM | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | Enterprise, soporte NVIDIA |
| SGLang | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | Outputs estructurados |

---

## NIVEL 3: RAG y Sistemas Agentic
> **Objetivo**: Construir sistemas RAG y agentes inteligentes  
> **Duraci√≥n**: 6-8 horas  
> **Prerequisitos**: Nivel 2 completado (al menos Ollama + vLLM)

### Lab 3.1: RAG Application in AI Workbench ‚ö†Ô∏è
| Aspecto | Detalle |
|---------|---------|
| **Tiempo** | 60 min |
| **Requisito** | NGC + Tavily API Keys |
| **Objetivo** | Entender RAG agentic de NVIDIA |
| **Aprender√°s** | Route-Evaluate-Iterate pattern, web search |
| **Entregable** | RAG funcional con AI Workbench |

**Conceptos clave a dominar**:
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                 AGENTIC RAG FLOW                ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Query ‚Üí Router ‚Üí [RAG | Web Search] ‚Üí LLM     ‚îÇ
‚îÇ                         ‚Üì                       ‚îÇ
‚îÇ              Evaluator (hallucination check)    ‚îÇ
‚îÇ                         ‚Üì                       ‚îÇ
‚îÇ              [Pass ‚Üí Response | Fail ‚Üí Retry]   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Lab 3.2: Text to Knowledge Graph ‚ö†Ô∏è
| Aspecto | Detalle |
|---------|---------|
| **Tiempo** | 60 min |
| **Requisito** | API Keys varios |
| **Objetivo** | Construir knowledge graphs desde texto |
| **Aprender√°s** | Entity extraction, relationship mapping, Neo4j |
| **Entregable** | Knowledge graph navegable |

**Por qu√© es importante**: Habilita hybrid retrieval (vector + graph) para Cerebro DS.

### Lab 3.3: Multi-Agent Chatbot ‚ö†Ô∏è
| Aspecto | Detalle |
|---------|---------|
| **Tiempo** | 90 min |
| **Requisito** | NGC/OpenAI Keys |
| **Objetivo** | Sistema multi-agente completo |
| **Aprender√°s** | Agent orchestration, tool use, memory |
| **Entregable** | Chatbot con m√∫ltiples agentes especializados |

**Arquitectura Multi-Agente**:
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                 ORCHESTRATOR                      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îÇ
‚îÇ  ‚îÇ Agent 1 ‚îÇ  ‚îÇ Agent 2 ‚îÇ  ‚îÇ Agent 3 ‚îÇ          ‚îÇ
‚îÇ  ‚îÇ Search  ‚îÇ  ‚îÇ Code    ‚îÇ  ‚îÇ Data    ‚îÇ          ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ
‚îÇ       ‚îÇ            ‚îÇ            ‚îÇ                ‚îÇ
‚îÇ       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îÇ
‚îÇ                    ‚Üì                             ‚îÇ
‚îÇ              Shared Memory                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### üìä Checkpoint Nivel 3
```
‚ñ° Entiendo el patr√≥n Route-Evaluate-Iterate
‚ñ° Puedo construir knowledge graphs desde documentos
‚ñ° S√© orquestar m√∫ltiples agentes
‚ñ° Entiendo cu√°ndo usar RAG vs Knowledge Graph vs Hybrid
‚ñ° Puedo implementar detecci√≥n de alucinaciones
```

---

## NIVEL 4: Fine-Tuning de Modelos
> **Objetivo**: Personalizar modelos para casos de uso espec√≠ficos  
> **Duraci√≥n**: 8-10 horas  
> **Prerequisitos**: Nivel 2 completado, HuggingFace Token

### Lab 4.1: Fine-tune with PyTorch ‚ö†Ô∏è
| Aspecto | Detalle |
|---------|---------|
| **Tiempo** | 90 min |
| **Requisito** | HuggingFace Token |
| **Objetivo** | Fine-tuning b√°sico con PyTorch nativo |
| **Aprender√°s** | Training loop, datasets, checkpointing |
| **Entregable** | Modelo fine-tuned con PyTorch |

**Por qu√© primero**: Entender los fundamentos antes de usar frameworks.

### Lab 4.2: Unsloth ‚ö†Ô∏è
| Aspecto | Detalle |
|---------|---------|
| **Tiempo** | 60 min |
| **Requisito** | HuggingFace Token |
| **Objetivo** | Fine-tuning 2x m√°s r√°pido con menos memoria |
| **Aprender√°s** | Unsloth optimizations, LoRA eficiente |
| **Entregable** | Fine-tuning r√°pido y eficiente |

### Lab 4.3: LLaMA Factory ‚ö†Ô∏è
| Aspecto | Detalle |
|---------|---------|
| **Tiempo** | 90 min |
| **Requisito** | HuggingFace Token |
| **Objetivo** | Fine-tuning con UI y m√∫ltiples m√©todos |
| **Aprender√°s** | LoRA, QLoRA, full fine-tuning, DPO, RLHF |
| **Entregable** | Modelo entrenado con LLaMA Factory |

**M√©todos de Fine-Tuning**:
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              FINE-TUNING METHODS                    ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Full Fine-tuning    ‚îÇ Actualiza todos los params  ‚îÇ
‚îÇ  LoRA                ‚îÇ Low-Rank Adaptation         ‚îÇ
‚îÇ  QLoRA               ‚îÇ LoRA + Quantization         ‚îÇ
‚îÇ  DPO                 ‚îÇ Direct Preference Opt       ‚îÇ
‚îÇ  RLHF                ‚îÇ Reinforcement Learning HF   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Lab 4.4: Fine-tune with NeMo ‚ö†Ô∏è
| Aspecto | Detalle |
|---------|---------|
| **Tiempo** | 120 min |
| **Requisito** | HuggingFace Token |
| **Objetivo** | Fine-tuning enterprise con NeMo AutoModel |
| **Aprender√°s** | NeMo framework, recipes, distributed training |
| **Entregable** | Modelo fine-tuned con NeMo |

**Modelos soportados por lab**:

| Lab | Modelos | Tama√±o M√°ximo |
|-----|---------|---------------|
| PyTorch | Cualquiera | ~8B (memoria) |
| Unsloth | Llama, Mistral | ~70B (QLoRA) |
| LLaMA Factory | 100+ modelos | ~70B |
| NeMo | Llama, Qwen, etc. | ~70B |

### üìä Checkpoint Nivel 4
```
‚ñ° Entiendo la diferencia entre LoRA, QLoRA, full fine-tuning
‚ñ° Puedo elegir el m√©todo correcto seg√∫n recursos/objetivo
‚ñ° S√© preparar datasets para fine-tuning
‚ñ° Puedo evaluar modelos fine-tuned
‚ñ° Entiendo trade-offs de cada framework
```

---

## NIVEL 5: Aplicaciones Especializadas
> **Objetivo**: Dominar casos de uso avanzados y espec√≠ficos  
> **Duraci√≥n**: 10-12 horas  
> **Prerequisitos**: Niveles 1-3 completados

### Track A: Generaci√≥n de Im√°genes

#### Lab 5A.1: Comfy UI
| Aspecto | Detalle |
|---------|---------|
| **Tiempo** | 45 min |
| **Objetivo** | UI node-based para generaci√≥n de im√°genes |
| **Aprender√°s** | Workflows visuales, pipelines de difusi√≥n |
| **Entregable** | Comfy UI funcional con workflows |

#### Lab 5A.2: FLUX.1 Dreambooth LoRA ‚ö†Ô∏è
| Aspecto | Detalle |
|---------|---------|
| **Tiempo** | 2-3 hrs |
| **Requisito** | HuggingFace Token, acceso FLUX.1-dev |
| **Objetivo** | Fine-tuning de modelos de difusi√≥n |
| **Aprender√°s** | Dreambooth, LoRA para im√°genes, multi-concept |
| **Entregable** | Modelo FLUX personalizado |

### Track B: Multi-Modal

#### Lab 5B.1: Multi-modal Inference
| Aspecto | Detalle |
|---------|---------|
| **Tiempo** | 60 min |
| **Objetivo** | Inferencia con modelos vision-language |
| **Aprender√°s** | VLMs, image understanding, visual QA |
| **Entregable** | Modelo multi-modal sirviendo |

#### Lab 5B.2: Live VLM WebUI
| Aspecto | Detalle |
|---------|---------|
| **Tiempo** | 60 min |
| **Objetivo** | UI para modelos vision-language en tiempo real |
| **Aprender√°s** | Streaming video, real-time inference |
| **Entregable** | VLM procesando video/webcam en vivo |

#### Lab 5B.3: Video Search & Summarization (VSS)
| Aspecto | Detalle |
|---------|---------|
| **Tiempo** | 90 min |
| **Objetivo** | B√∫squeda y resumen de video |
| **Aprender√°s** | Video embeddings, temporal search |
| **Entregable** | Sistema VSS funcional |

### Track C: Rob√≥tica y Simulaci√≥n

#### Lab 5C.1: Isaac Sim
| Aspecto | Detalle |
|---------|---------|
| **Tiempo** | 60 min |
| **Objetivo** | Simulaci√≥n rob√≥tica fotorealista |
| **Aprender√°s** | Omniverse, physics simulation |
| **Entregable** | Isaac Sim corriendo simulaciones |

#### Lab 5C.2: Isaac Lab
| Aspecto | Detalle |
|---------|---------|
| **Tiempo** | 90 min |
| **Objetivo** | RL para rob√≥tica |
| **Aprender√°s** | Reinforcement learning, locomotion |
| **Entregable** | Agente RL entrenado en simulaci√≥n |

### Track D: Data Science & Ciencia

#### Lab 5D.1: CUDA-X Data Science
| Aspecto | Detalle |
|---------|---------|
| **Tiempo** | 60 min |
| **Objetivo** | Data science acelerado por GPU |
| **Aprender√°s** | cuDF, cuML, cuGraph, RAPIDS |
| **Entregable** | Pipeline de datos GPU-accelerated |

#### Lab 5D.2: Portfolio Optimization
| Aspecto | Detalle |
|---------|---------|
| **Tiempo** | 60 min |
| **Objetivo** | Optimizaci√≥n financiera con GPU |
| **Aprender√°s** | Finanzas cuantitativas, optimization |
| **Entregable** | Optimizer de portfolio funcional |

#### Lab 5D.3: Single-cell RNA Sequencing
| Aspecto | Detalle |
|---------|---------|
| **Tiempo** | 90 min |
| **Objetivo** | An√°lisis de secuenciaci√≥n celular |
| **Aprender√°s** | Bioinform√°tica, scRNA-seq analysis |
| **Entregable** | Pipeline de an√°lisis celular |

### Track E: Optimizaci√≥n Avanzada

#### Lab 5E.1: Optimized JAX
| Aspecto | Detalle |
|---------|---------|
| **Tiempo** | 60 min |
| **Objetivo** | JAX optimizado para DGX Spark |
| **Aprender√°s** | JAX, XLA compilation, JIT |
| **Entregable** | JAX corriendo con optimizaciones |

### üìä Checkpoint Nivel 5
```
‚ñ° Complet√© al menos 2 tracks completos
‚ñ° Puedo elegir herramientas seg√∫n el dominio
‚ñ° Entiendo las capacidades multi-modales
‚ñ° S√© cu√°ndo usar GPU-acceleration para data science
```

---

## NIVEL 6: Multi-Node y Distribuido
> **Objetivo**: Escalar a m√∫ltiples DGX Sparks  
> **Duraci√≥n**: 4-6 horas  
> **Prerequisitos**: Todos los niveles anteriores + 2 DGX Sparks

### Lab 6.1: Connect Two Sparks ‚ùå
| Aspecto | Detalle |
|---------|---------|
| **Tiempo** | 60 min |
| **Requisito** | 2 DGX Spark + cable QSFP |
| **Objetivo** | Conectar dos sistemas via 200GbE |
| **Aprender√°s** | Networking multi-node, SSH keys |
| **Entregable** | Dos Sparks comunic√°ndose |

### Lab 6.2: NCCL for Two Sparks ‚ùå
| Aspecto | Detalle |
|---------|---------|
| **Tiempo** | 60 min |
| **Requisito** | Lab 6.1 completado |
| **Objetivo** | Comunicaci√≥n GPU-GPU entre nodos |
| **Aprender√°s** | NCCL, MPI, collective operations |
| **Entregable** | NCCL tests pasando entre nodos |

### Lab 6.3: Distributed Training
| Aspecto | Detalle |
|---------|---------|
| **Tiempo** | 120 min |
| **Objetivo** | Entrenamiento distribuido multi-node |
| **Aprender√°s** | Data/Model parallelism, DeepSpeed |
| **Entregable** | Training job distribuido funcionando |

### üìä Checkpoint Nivel 6
```
‚ñ° Puedo conectar m√∫ltiples DGX Sparks
‚ñ° Entiendo NCCL y comunicaci√≥n GPU-GPU
‚ñ° Puedo ejecutar training distribuido
‚ñ° S√© cu√°ndo escalar horizontalmente
```

---

## Rutas de Especializaci√≥n Recomendadas

### üéØ Ruta: RAG & Agentes (Cerebro DS)
```
Nivel 1 (completo)
    ‚Üì
Nivel 2: Ollama ‚Üí vLLM ‚Üí NIM
    ‚Üì
Nivel 3: RAG Workbench ‚Üí txt2kg ‚Üí Multi-Agent
    ‚Üì
Nivel 4: NeMo (para fine-tuning especializado)
    ‚Üì
Nivel 5D: CUDA-X Data Science
```
**Duraci√≥n**: ~25 horas

### üéØ Ruta: MLOps & Producci√≥n
```
Nivel 1 (completo)
    ‚Üì
Nivel 2: Ollama ‚Üí vLLM ‚Üí TRT-LLM ‚Üí NIM
    ‚Üì
Nivel 4: NeMo ‚Üí Unsloth
    ‚Üì
Nivel 5E: JAX Optimized
    ‚Üì
Nivel 6 (si tienes 2 Sparks)
```
**Duraci√≥n**: ~30 horas

### üéØ Ruta: Generaci√≥n Creativa
```
Nivel 1 (completo)
    ‚Üì
Nivel 2: Ollama ‚Üí vLLM
    ‚Üì
Nivel 5A: Comfy UI ‚Üí FLUX Fine-tuning
    ‚Üì
Nivel 5B: Multi-modal ‚Üí Live VLM ‚Üí VSS
```
**Duraci√≥n**: ~20 horas

### üéØ Ruta: Data Science
```
Nivel 1 (completo)
    ‚Üì
Nivel 2: Ollama
    ‚Üì
Nivel 5D: CUDA-X ‚Üí Portfolio ‚Üí Single-cell
    ‚Üì
Nivel 3: RAG Workbench ‚Üí txt2kg
```
**Duraci√≥n**: ~20 horas

### üéØ Ruta: Rob√≥tica
```
Nivel 1 (completo)
    ‚Üì
Nivel 2: Ollama ‚Üí vLLM
    ‚Üì
Nivel 5C: Isaac Sim ‚Üí Isaac Lab
    ‚Üì
Nivel 5B: Multi-modal ‚Üí Live VLM
```
**Duraci√≥n**: ~15 horas

---

## Calendario Sugerido

### Opci√≥n A: Intensivo (2 semanas, 4h/d√≠a)
```
Semana 1:
‚îú‚îÄ‚îÄ D√≠a 1-2: Nivel 1 (Fundamentos)
‚îú‚îÄ‚îÄ D√≠a 3-5: Nivel 2 (Inferencia)
‚îú‚îÄ‚îÄ D√≠a 6-7: Nivel 3 (RAG & Agentes)

Semana 2:
‚îú‚îÄ‚îÄ D√≠a 8-10: Nivel 4 (Fine-tuning)
‚îú‚îÄ‚îÄ D√≠a 11-14: Nivel 5 (Especializaci√≥n - elegir track)
```

### Opci√≥n B: Regular (4 semanas, 2h/d√≠a)
```
Semana 1: Nivel 1 + Nivel 2 (Ollama, vLLM)
Semana 2: Nivel 2 (TRT-LLM, NIM, SGLang) + Nivel 3
Semana 3: Nivel 4 (Fine-tuning)
Semana 4: Nivel 5 (Track elegido)
```

### Opci√≥n C: Relajado (8 semanas, 1h/d√≠a)
```
Semana 1-2: Nivel 1
Semana 3-4: Nivel 2
Semana 5: Nivel 3
Semana 6-7: Nivel 4
Semana 8: Nivel 5
```

---

## M√©tricas de Progreso

### Por Nivel
| Nivel | Labs | Horas | % Acumulado |
|-------|------|-------|-------------|
| 1 | 5 | 3-4h | 8% |
| 2 | 9 | 6-8h | 25% |
| 3 | 3 | 6-8h | 42% |
| 4 | 4 | 8-10h | 63% |
| 5 | 11 | 10-12h | 92% |
| 6 | 3 | 4-6h | 100% |

### Tracking Personal
```markdown
## Mi Progreso

### Nivel 1: Fundamentos
- [ ] 1.1 Connect to Your Spark
- [ ] 1.2 DGX Dashboard
- [ ] 1.3 VS Code Remote
- [ ] 1.4 Tailscale VPN
- [ ] 1.5 Vibe Coding

### Nivel 2: Inferencia
- [ ] 2.1 Ollama
- [ ] 2.2 Open WebUI
- [ ] 2.3 vLLM
- [ ] 2.4 TensorRT-LLM
- [ ] 2.5 NIM on Spark
- [ ] 2.6 SGLang
- [ ] 2.7 Nemotron
- [ ] 2.8 Speculative Decoding
- [ ] 2.9 NVFP4 Quantization

### Nivel 3: RAG & Agentes
- [ ] 3.1 RAG AI Workbench
- [ ] 3.2 Text to Knowledge Graph
- [ ] 3.3 Multi-Agent Chatbot

### Nivel 4: Fine-Tuning
- [ ] 4.1 PyTorch Fine-tune
- [ ] 4.2 Unsloth
- [ ] 4.3 LLaMA Factory
- [ ] 4.4 NeMo Fine-tune

### Nivel 5: Especializaci√≥n
Track elegido: ________________
- [ ] Lab 5.X.1
- [ ] Lab 5.X.2
- [ ] Lab 5.X.3

### Nivel 6: Multi-Node (Opcional)
- [ ] 6.1 Connect Two Sparks
- [ ] 6.2 NCCL
- [ ] 6.3 Distributed Training
```

---

## Recursos Adicionales

### Documentaci√≥n Oficial
- [DGX Spark User Guide](https://docs.nvidia.com/dgx/dgx-spark/)
- [NVIDIA Developer Forums](https://forums.developer.nvidia.com/c/accelerated-computing/dgx-spark-gb10)
- [NGC Catalog](https://catalog.ngc.nvidia.com/)

### Comunidad
- [NVIDIA Developer Discord](https://discord.gg/nvidia)
- [r/LocalLLaMA](https://reddit.com/r/LocalLLaMA)

### Cursos Complementarios
- [NVIDIA Deep Learning Institute](https://www.nvidia.com/en-us/training/)
- [Hugging Face Course](https://huggingface.co/course)

---

## Changelog

| Fecha | Cambio |
|-------|--------|
| 2026-01-16 | Plan de aprendizaje inicial |

---

*Plan dise√±ado para maximizar el aprendizaje progresivo y la retenci√≥n de conocimientos en DGX Spark*
